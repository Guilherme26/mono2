{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import itertools\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "from scipy.stats import kendalltau, pearsonr\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "sys.path.insert(1, '/home/guilherme-resende/Desktop/mono2/utils')\n",
    "import qif\n",
    "from categorize import Categorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"adult\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = yaml.load(open(\"datasets.yaml\"))\n",
    "df = pd.read_csv(datasets[ds_name][\"path\"])\n",
    "\n",
    "pre_process = Categorize(\n",
    "    df,\n",
    "    binary_cols=datasets[ds_name][\"binary_cols\"],\n",
    "    hierarchical_continuous_cols=datasets[ds_name][\"hierarchical_continuous_cols\"],\n",
    "    non_hierarchical_cols=datasets[ds_name][\"non_hierarchical_cols\"]\n",
    ")\n",
    "\n",
    "df = pre_process.transform_data()\n",
    "\n",
    "df_train = df.loc[df.set == \"train\"].drop(\"set\", axis=1)\n",
    "df_test = df.loc[df.set == \"test\"].drop(\"set\", axis=1)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll consider dividing the training set into training and validation.\n",
    "# The final model will be trained on the entire training set and tested on test set\n",
    "\n",
    "X = df_train.drop(\"targets\", axis=1)\n",
    "Y = df_train.targets\n",
    "\n",
    "X_test = df_test.drop(\"targets\", axis=1)\n",
    "Y_test = df_test.targets.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth=8, n_estimators=128)\n",
    "xgb.fit(X.values, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb.predict(X_test.values)\n",
    "preds_proba = xgb.predict_proba(X_test.values)[:, 1:]\n",
    "df_test[\"preds_proba\"] = preds_proba\n",
    "\n",
    "print(\"Metrics:\")\n",
    "print(\"\\tF1-Score ->\", f1_score(Y_test, preds))\n",
    "print(\"\\tAUC ->\", roc_auc_score(Y_test, preds_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the SHAP scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb)\n",
    "shap_values = explainer.shap_values(X_test.values)\n",
    "\n",
    "shap_values = shap_values.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the QIF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize the probabilities\n",
    "df_test[\"preds_proba\"] = (df_test[\"preds_proba\"] * 100).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_leakage = qif.BayesLeakage(df_test)\n",
    "feature_names = X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "qif_values = []\n",
    "for feature in feature_names:\n",
    "    leakage = bayes_leakage.compute_flows(x=feature, y='preds_proba')\n",
    "    qif_values.append(max(leakage))\n",
    "\n",
    "qif_values = np.array(qif_values)\n",
    "qif_values = qif_values / qif_values.sum() # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qif_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_coefs_qif = {name: coef for name, coef in zip(feature_names, qif_values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression()\n",
    "logit.fit(X.values, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logit.predict(X_test.values)\n",
    "preds_proba = logit.predict_proba(X_test.values)[:, 1:]\n",
    "df_test[\"preds_proba\"] = preds_proba\n",
    "\n",
    "print(\"Metrics:\")\n",
    "print(\"\\tF1-Score ->\", f1_score(Y_test, preds))\n",
    "print(\"\\tAUC ->\", roc_auc_score(Y_test, preds_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_values = vars(logit)[\"coef_\"].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_coefs_logit = {name: coef for name, coef in zip(feature_names, logit_values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison Between Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_coefs_logit = dict(\n",
    "    sorted(\n",
    "        feat_coefs_logit.items(),\n",
    "        key=lambda item: abs(item[1]),\n",
    "        reverse=True\n",
    "    )\n",
    ")\n",
    "\n",
    "feat_coefs_qif = dict(\n",
    "    sorted(\n",
    "        feat_coefs_qif.items(),\n",
    "        key=lambda item: abs(item[1]),\n",
    "        reverse=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_at_k = []\n",
    "\n",
    "for k in range(2, len(feat_coefs_logit)):\n",
    "    jaccard_at_k.append(\n",
    "        jaccard_similarity_score(\n",
    "            list(feat_coefs_logit.keys())[:k],\n",
    "            list(feat_coefs_qif.keys())[:k]\n",
    "        )\n",
    "    )\n",
    "\n",
    "jaccard_at_k = np.round(jaccard_at_k, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(2, len(feat_coefs_logit))\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(12,7))\n",
    "\n",
    "plt.plot(x, jaccard_at_k, label=\"Real\")\n",
    "plt.plot(x, np.array(x)/len(feat_coefs_logit), linestyle=\"--\", label=\"Ideal\")\n",
    "plt.title(\"Jaccard Score @ K\", fontsize=16)\n",
    "plt.xlabel(\"K\", fontsize=14)\n",
    "plt.ylabel(\"Jaccard Score\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_at_k = []\n",
    "\n",
    "for k in range(2, len(feat_coefs_logit)):\n",
    "    intersection_at_k.append(\n",
    "        len(\n",
    "            set(\n",
    "                list(feat_coefs_logit.keys())[:k]\n",
    "            ).intersection(list(feat_coefs_qif.keys())[:k])\n",
    "        )\n",
    "    )\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(12,7))\n",
    "\n",
    "plt.plot(range(2, len(feat_coefs_logit)), intersection_at_k, label=\"Real\")\n",
    "plt.plot(range(2, len(feat_coefs_logit)), range(2, len(feat_coefs_logit)), linestyle=\"--\", label=\"Ideal\")\n",
    "plt.title(\"Intersection @ K\", fontsize=16)\n",
    "plt.xlabel(\"K\", fontsize=14)\n",
    "plt.ylabel(\"Intersection Length\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_coef, _ = kendalltau(qif_values, logit_values)\n",
    "p_coef, _ = pearsonr(qif_values, logit_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kendall:\", k_coef)\n",
    "print(\"Pearson:\", p_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
